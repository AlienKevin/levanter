eval_harness:
 task_spec: ["gsm8k"]
 max_eval_length: 2048
#  max_examples: 1
tokenizer: meta-llama/Meta-Llama-3-8B
model:
  type: llama
checkpoint_path: meta-llama/Meta-Llama-3-8B
checkpoint_is_hf: true
trainer:
  mp: p=bfloat16,c=bfloat16
  profiler: true

  per_device_parallelism: -1
  train_batch_size: 512

  tensor_parallel_axes: ["mlp", "heads"]
  fsdp_axis: "embed"
  batch_axis: "batch"
  ray:
    auto_start_cluster: false
